{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dog_Bread_Identification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8CYrZNkXdbL"
      },
      "source": [
        "# Dog Breed Identification\r\n",
        "This notebook uses transfer learning to build a Multi-Class Image Classifier using TensorFlow 2.x and TensorFlow Hub.\r\n",
        "\r\n",
        "## 1. Problem\r\n",
        "\r\n",
        "We are provided with a training set and a test set of images of dogs. Each image has a filename that is its unique id. The dataset comprises 120 breeds of dogs. The goal is to create a classifier capable of determining a dog's breed from a photo.\r\n",
        "\r\n",
        "## 2. Data\r\n",
        "\r\n",
        "The data we're using is from Kaggle's dog breed identification competition.\r\n",
        "\r\n",
        "https://www.kaggle.com/c/dog-breed-identification/data \r\n",
        "\r\n",
        "## 3. Evaluation\r\n",
        "\r\n",
        "The evaluation is a file with prediction probabilities for each dog breed of each test image.\r\n",
        "\r\n",
        "https://www.kaggle.com/c/dog-breed-identification/overview/evaluation\r\n",
        "\r\n",
        "## 4. Features\r\n",
        "* There are 120 breeds of dogs (this means there are 120 different classes).\r\n",
        "* There are around 10,222 images in the training set (these images have labels.\r\n",
        "* There are around 10,222 images in the test set (these images have no labels, because we'll want to predict them)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWfDkGL2ZB2D"
      },
      "source": [
        "### Getting the Data and Importing the Libraries\r\n",
        "We will start of by getting the data from Kaggle, using the Kaggle api but will do a pip \"force install\" first in order to prevent any errors. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnOb0H00Ugnt"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkbSR38WZ8HR"
      },
      "source": [
        "import os\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import datetime\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "from keras.utils import to_categorical\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "print(tf.__version__) #Make sure Tensorflow 2 is imported"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onhgemTszqNK"
      },
      "source": [
        "# Adding the Username and Key from the Kaggle Token Folder\r\n",
        "os.environ['KAGGLE_USERNAME']=\"hassanjoumaa\"\r\n",
        "os.environ['KAGGLE_KEY']=\"5e66163ab8d43def76ee3643557bea64\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gltwOmQ8KbS"
      },
      "source": [
        "# Downloading the Dataset from Kaggle\r\n",
        "!kaggle competitions download -c dog-breed-identification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxqOzkbr8LH2"
      },
      "source": [
        "# Unziping the Folder\r\n",
        "!unzip dog-breed-identification.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvRGO1HeVmoc"
      },
      "source": [
        "# Importing the labels.csv to a pandas dataframe\r\n",
        "df = pd.read_csv(\"./labels.csv\")\r\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRokRsrfWRqI"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MHGapevWYnJ"
      },
      "source": [
        "#Viewing the data distribution\r\n",
        "df[\"breed\"].value_counts().plot.bar(figsize=(20,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLl1FQwwbvQW"
      },
      "source": [
        "### Preprocessing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StWydJHYXO6i"
      },
      "source": [
        "# Geting the list of training filenames\r\n",
        "filenames = ['/content/train/' + fname +'.jpg' for fname in df[\"id\"]]\r\n",
        "print(filenames[:10])\r\n",
        "if len(os.listdir('/content/train')) == len(filenames):\r\n",
        "  print(\"Amount of files match!\")\r\n",
        "else:\r\n",
        "  print(\"Amount of files don't match!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk2oSLEsZb67"
      },
      "source": [
        "# Getting the unique labels and the list of breeds\r\n",
        "labels = list(df[\"breed\"])\r\n",
        "unique_breeds = np.unique(labels)\r\n",
        "print(\"Labels:\",labels[:10],\"\\n\")\r\n",
        "print(\"Unique Breads:\",unique_breeds[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GJNp_Ftb77q"
      },
      "source": [
        "> One Hot Encoding the Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObwEXTQNcyOd"
      },
      "source": [
        "lbl=LabelEncoder()\r\n",
        "labels=lbl.fit_transform(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6-n0jyOezhM"
      },
      "source": [
        "labels = to_categorical(labels)\r\n",
        "print(\"Number of unique labels:\",len(labels[0]))\r\n",
        "labels[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikrAddRlfAAs"
      },
      "source": [
        "X = filenames\r\n",
        "y = labels\r\n",
        "len(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UTWMtjIcIMl"
      },
      "source": [
        "> Splitting the data into Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqHqMJTtheHH"
      },
      "source": [
        "NUM_IMAGES = 1000 #@param {type:\"slider\", min:1000, max:10222, step:2}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc1qLMQBicGw"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],\r\n",
        "                                                  y[:NUM_IMAGES],\r\n",
        "                                                  test_size=0.2,\r\n",
        "                                                  random_state=42)\r\n",
        "\r\n",
        "len(X_train), len(X_val), len(y_train), len(y_val) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1eEG4GCceKT"
      },
      "source": [
        "### ***Getting the Data Ready in Batches***\r\n",
        "\r\n",
        "We will define some functions to process the images and to put the data in Batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1dsH_29jPgH"
      },
      "source": [
        "# Define image size\r\n",
        "IMG_SIZE = 224\r\n",
        "\r\n",
        "# Create a function for preprocessing images\r\n",
        "def process_image(image_path, img_size=IMG_SIZE):\r\n",
        "  \"\"\"\r\n",
        "  Takes an image file path and turns the image into a Tensor.\r\n",
        "  \"\"\"\r\n",
        "  # Read in an image file\r\n",
        "  image = tf.io.read_file(image_path)\r\n",
        "  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\r\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\r\n",
        "  # Convert the colour channel values from 0-255 to 0-1 values\r\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\r\n",
        "  # Resize the image to our desired value (224, 224)\r\n",
        "  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\r\n",
        "\r\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re7BrkDlRqqE"
      },
      "source": [
        "# Create a simple function to return a tuple (image, label)\r\n",
        "def get_image_label(image_path, label):\r\n",
        "  \"\"\"\r\n",
        "  Takes an image file path name and the assosciated label,\r\n",
        "  processes the image and reutrns a tuple of (image, label).\r\n",
        "  \"\"\"\r\n",
        "  image = process_image(image_path)\r\n",
        "  return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvZkFwaoRvEX"
      },
      "source": [
        "# Define the batch size\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "# Create a function to turn data into batches\r\n",
        "def create_data_batches(X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\r\n",
        "  \"\"\"\r\n",
        "  Creates batches of data out of image (X) and label (y) pairs.\r\n",
        "  Shuffles the data if it's training data but doesn't shuffle if it's validation data.\r\n",
        "  Also accepts test data as input (no labels).\r\n",
        "  \"\"\"\r\n",
        "  # If the data is a test dataset\r\n",
        "  if test_data:\r\n",
        "    print(\"Creating test data batches...\")\r\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X))) # only filepaths (no labels)\r\n",
        "    data_batch = data.map(process_image).batch(BATCH_SIZE)\r\n",
        "    return data_batch\r\n",
        "  \r\n",
        "  # If the data is a valid dataset\r\n",
        "  elif valid_data:\r\n",
        "    print(\"Creating validation data batches...\")\r\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X), # filepaths\r\n",
        "                                               tf.constant(y))) # labels\r\n",
        "    data_batch = data.map(get_image_label).batch(BATCH_SIZE)\r\n",
        "    return data_batch\r\n",
        "\r\n",
        "  else:\r\n",
        "    print(\"Creating training data batches...\")\r\n",
        "    # Turn filepaths and labels into Tensors\r\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\r\n",
        "                                               tf.constant(y)))\r\n",
        "    # Shuffling pathnames and labels before mapping image processor function is faster than shuffling images\r\n",
        "    data = data.shuffle(buffer_size=len(X))\r\n",
        "\r\n",
        "    # Create (image, label) tuples (this also turns the image path into a preprocessed image)\r\n",
        "    data = data.map(get_image_label)\r\n",
        "\r\n",
        "    # Turn the training data into batches\r\n",
        "    data_batch = data.batch(BATCH_SIZE)\r\n",
        "  return data_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1rJMfnDvrzZ"
      },
      "source": [
        "# Create training and validation data batches\r\n",
        "train_data = create_data_batches(X_train, y_train)\r\n",
        "val_data = create_data_batches(X_val, y_val, valid_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98naRkLMz-3K"
      },
      "source": [
        "# Check out the different attributes of our data batches\r\n",
        "train_data.element_spec, val_data.element_spec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUwahpMfGeub"
      },
      "source": [
        "# Create a function for viewing images in a data batch\r\n",
        "def show_25_images(images, labels):\r\n",
        "  \"\"\"\r\n",
        "  Displays a plot of 25 images and their labels from a data batch.\r\n",
        "  \"\"\"\r\n",
        "  # Setup the figure\r\n",
        "  plt.figure(figsize=(15, 15))\r\n",
        "  # Loop through 25 (for displaying 25 images)\r\n",
        "  for i in range(25):\r\n",
        "    # Create subplots (5 rows, 5 columns)\r\n",
        "    ax = plt.subplot(5, 5, i+1)\r\n",
        "    # Display an image \r\n",
        "    plt.imshow(images[i])\r\n",
        "    # Add the image label as the title\r\n",
        "    plt.title(unique_breeds[labels[i].argmax()])\r\n",
        "    # Turn the grid lines off\r\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLucM-2rHZg1"
      },
      "source": [
        "# Now let's visualize the data in a training batch\r\n",
        "train_images, train_labels = next(train_data.as_numpy_iterator())\r\n",
        "show_25_images(train_images, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcthEA3vddWR"
      },
      "source": [
        "### Creating the Model\r\n",
        "We will be using the imagenet-mobilenet_v2_130_224-classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaFa_jToIYkG"
      },
      "source": [
        "INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3]\r\n",
        "OUTPUT_SHAPE = len(unique_breeds)\r\n",
        "MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlInysQJf_6B"
      },
      "source": [
        "# Setup the model layers\r\n",
        "model = tf.keras.Sequential([\r\n",
        "    hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)\r\n",
        "    tf.keras.layers.Dense(units=OUTPUT_SHAPE,activation=\"softmax\") # Layer 2 (output layer)\r\n",
        "  ])\r\n",
        "\r\n",
        "# Compile the model\r\n",
        "model.compile(\r\n",
        "      loss=tf.keras.losses.CategoricalCrossentropy(),\r\n",
        "      optimizer=tf.keras.optimizers.Adam(),\r\n",
        "      metrics=[\"accuracy\"]\r\n",
        "  )\r\n",
        "\r\n",
        "# Build the model\r\n",
        "model.build(INPUT_SHAPE)\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8wI4PS6kcWM"
      },
      "source": [
        "# Load TensorBoard notebook extension\r\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dpquDXIyznB"
      },
      "source": [
        "!mkdir ./logs\r\n",
        "# Create a function to build a TensorBoard callback\r\n",
        "def create_tensorboard_callback():\r\n",
        "  # Create a log directory for storing TensorBoard logs\r\n",
        "  logdir = os.path.join(\"./logs\",\r\n",
        "                        # Make it so the logs get tracked whenever we run an experiment\r\n",
        "                        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\n",
        "  return tf.keras.callbacks.TensorBoard(logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SY6_PzJ0pbZ"
      },
      "source": [
        "# Create early stopping callback\r\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\",\r\n",
        "                                                  patience=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwioYq4W2roR"
      },
      "source": [
        "NUM_EPOCHS = 100\r\n",
        "tensorboard = create_tensorboard_callback()\r\n",
        "\r\n",
        "# Fit the model to the data passing it the callbacks we created\r\n",
        "model.fit(x=train_data,\r\n",
        "          epochs=NUM_EPOCHS,\r\n",
        "          validation_data=val_data,\r\n",
        "          validation_freq=1,\r\n",
        "          callbacks=[tensorboard, early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkPZuaJeeG8g"
      },
      "source": [
        "### Viewing the performance of our Model\r\n",
        "Clearly our model is currently overfitting but training on the full dataset (using more data) might fix the problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbjnY7TC6FcO"
      },
      "source": [
        "#Uncomment below to view the Tensorboard\r\n",
        "#%tensorboard --logdir /content/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWMryHcc9M9A"
      },
      "source": [
        "predictions = model.predict(val_data, verbose=1)\r\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgACiEFh9SIu"
      },
      "source": [
        "# Turn prediction probabilities into their respective label (easier to understand)\r\n",
        "def get_pred_label(prediction_probabilities):\r\n",
        "  \"\"\"\r\n",
        "  Turns an array of prediction probabilities into a label.\r\n",
        "  \"\"\"\r\n",
        "  return unique_breeds[np.argmax(prediction_probabilities)]\r\n",
        "\r\n",
        "# Get a predicted label based on an array of prediction probabilities\r\n",
        "pred_label = get_pred_label(predictions[81])\r\n",
        "pred_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz1A42zi99K0"
      },
      "source": [
        "# Create a function to unbatch a batch dataset\r\n",
        "def unbatchify(data):\r\n",
        "  \"\"\"\r\n",
        "  Takes a batched dataset of (image, label) Tensors and reutrns separate arrays\r\n",
        "  of images and labels.\r\n",
        "  \"\"\"\r\n",
        "  images = []\r\n",
        "  labels = []\r\n",
        "  # Loop through unbatched data\r\n",
        "  for image, label in data.unbatch().as_numpy_iterator():\r\n",
        "    images.append(image)\r\n",
        "    labels.append(unique_breeds[np.argmax(label)])\r\n",
        "  return images, labels\r\n",
        "\r\n",
        "# Unbatchify the validation data\r\n",
        "val_images, val_labels = unbatchify(val_data)\r\n",
        "val_images[0], val_labels[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZbTeJG1-4EQ"
      },
      "source": [
        "def plot_pred(prediction_probabilities, labels, images, n=1):\r\n",
        "  \"\"\"\r\n",
        "  View the prediction, ground truth and image for sample n\r\n",
        "  \"\"\"\r\n",
        "  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]\r\n",
        "\r\n",
        "  # Get the pred label\r\n",
        "  pred_label = get_pred_label(pred_prob)\r\n",
        "\r\n",
        "  # Plot image & remove ticks\r\n",
        "  plt.imshow(image)\r\n",
        "  plt.xticks([])\r\n",
        "  plt.yticks([])\r\n",
        "\r\n",
        "  # Change the colour of the title depending on if the prediction is right or wrong\r\n",
        "  if pred_label == true_label:\r\n",
        "    color = \"green\"\r\n",
        "  else:\r\n",
        "    color = \"red\"\r\n",
        "  \r\n",
        "  # Change plot title to be predicted, probability of prediction and truth label\r\n",
        "  plt.title(\"{} {:2.0f}% {}\".format(pred_label,\r\n",
        "                                    np.max(pred_prob)*100,\r\n",
        "                                    true_label),\r\n",
        "                                    color=color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QN1b0FLT_A5o"
      },
      "source": [
        "plot_pred(prediction_probabilities=predictions,\r\n",
        "          labels=val_labels,\r\n",
        "          images=val_images,\r\n",
        "          n=77)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njrXoiCNejmE"
      },
      "source": [
        "### Saving our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4S3dvGT_KBR"
      },
      "source": [
        "# Create a function to save a model\r\n",
        "!mkdir ./models\r\n",
        "def save_model(model, suffix=None):\r\n",
        "  \"\"\"\r\n",
        "  Saves a given model in a models directory and appends a suffix (string).\r\n",
        "  \"\"\"\r\n",
        "  # Create a model directory pathname with current time\r\n",
        "  modeldir = os.path.join(\"./models\",\r\n",
        "                          datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\"))\r\n",
        "  model_path = modeldir + \"-\" + suffix + \".h5\" # save format of model\r\n",
        "  print(f\"Saving model to: {model_path}...\")\r\n",
        "  model.save(model_path)\r\n",
        "  return model_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVKW6XTf_g37"
      },
      "source": [
        "# Create a function to load a trained model\r\n",
        "def load_model(model_path):\r\n",
        "  \"\"\"\r\n",
        "  Loads a saved model from a specified path.\r\n",
        "  \"\"\"\r\n",
        "  print(f\"Loading saved model from: {model_path}\")\r\n",
        "  model = tf.keras.models.load_model(model_path, \r\n",
        "                                     custom_objects={\"KerasLayer\":hub.KerasLayer})\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4TCEyZ6D2t1"
      },
      "source": [
        "# Save our model trained on 1000 images\r\n",
        "#save_model(model, suffix=\"1000-images-mobilenetv2-Adam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6AlDTTxD5JD"
      },
      "source": [
        "# Load a trained model\r\n",
        "#loaded_1000_image_model = load_model('PATH') #Provide the PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSzgeEmdepiD"
      },
      "source": [
        "### Training on the Full Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtBcyF6gFMHX"
      },
      "source": [
        "len(X), len(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09QdrLbLFRrD"
      },
      "source": [
        "# Create a data batch with the full data set\r\n",
        "full_data = create_data_batches(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyrU_XIYEyn1"
      },
      "source": [
        "# Setup the model layers\r\n",
        "Final_model = tf.keras.Sequential([\r\n",
        "    hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)\r\n",
        "    tf.keras.layers.Dense(units=OUTPUT_SHAPE,activation=\"softmax\") # Layer 2 (output layer)\r\n",
        "  ])\r\n",
        "\r\n",
        "# Compile the model\r\n",
        "Final_model.compile(\r\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\r\n",
        "    optimizer=tf.keras.optimizers.Adam(),\r\n",
        "    metrics=[\"accuracy\"]\r\n",
        "  )\r\n",
        "\r\n",
        "# Build the model\r\n",
        "Final_model.build(INPUT_SHAPE)\r\n",
        "Final_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeZpSeGcFvLQ"
      },
      "source": [
        "# Create final model callbacks\r\n",
        "Final_model_tensorboard = create_tensorboard_callback()\r\n",
        "Final_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\",\r\n",
        "                                                             patience=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRUh3VHtF9al"
      },
      "source": [
        "# Fit the final model to the full data\r\n",
        "NUM_EPOCHS=12\r\n",
        "Final_model.fit(x=full_data,\r\n",
        "               epochs=NUM_EPOCHS,\r\n",
        "               callbacks=[Final_model_tensorboard, Final_model_early_stopping])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sFIg2QyGVic"
      },
      "source": [
        "save_model(Final_model, suffix=\"full-dataset-mobilenetv2-Adam\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E7G2wj8fC2U"
      },
      "source": [
        "### Creating the submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0iXqjCRHXHq"
      },
      "source": [
        "# Load test image filenames\r\n",
        "test_path = \"/content/test/\"\r\n",
        "test_filenames = [test_path + fname for fname in os.listdir(test_path)]\r\n",
        "print(len(test_filenames))\r\n",
        "test_filenames[:10]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Xuw4K5Hbfd"
      },
      "source": [
        "# Create test data batch\r\n",
        "test_data = create_data_batches(test_filenames, test_data=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErHoYBvnIC1b"
      },
      "source": [
        "test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af4Ec1jGIFyM"
      },
      "source": [
        "# Make predictions on test data batch using the loaded final model\r\n",
        "test_predictions = Final_model.predict(test_data,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBRL5quRIIyR"
      },
      "source": [
        "# Create a pandas DataFrame with empty columns\r\n",
        "preds_df = pd.DataFrame(columns=[\"id\"] + list(unique_breeds))\r\n",
        "preds_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeDoowrxIWAF"
      },
      "source": [
        "# Append test image ID's to predictions DataFrame\r\n",
        "test_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]\r\n",
        "preds_df[\"id\"] = test_ids\r\n",
        "preds_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_wRJOaag2T1"
      },
      "source": [
        "# Add the prediction probabilities to each dog breed column\r\n",
        "preds_df[list(unique_breeds)] = test_predictions\r\n",
        "preds_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keJPNTTDg5Hc"
      },
      "source": [
        "# Save our predictions dataframe to CSV for submission to Kaggle\r\n",
        "preds_df.to_csv(\"./full_model_predictions_submission.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}